---
title: "Using RAIN to Assess Periodicity in Time Series"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## We want to assess periodicity in individual channels
To do this, we'll be using rain, a method from Thaben and Westermark 2014. 
-We like this method because it is nonparametric, therefore any wave form can be detected
-Fitting harmonic oscillators is problematic because not all periodic biological time series are symmetrical, which, although you could fit an oscillator and it may be significant, will inherently be misinformative about the actual temporal dynamics occurring. 
-Rain does this by essentially doing a mountain of mann-whitney tests between the ranks of the measurements in each consecutive timepoint in your timeseries (ie, test H0: ranks(X_{t1})=ranks(X_{t2})) and see if there is a consistent pattern of ranks increasing then decreasing (or decreasing then increasing). 

```{r Setting Up}
# install.packages("compositions")
# install.packages("pracma")
# install.packages("rain")
library(compositions)
library(pracma)
library(rain)
library(tidyverse)

# Import test data (derived from OTU table)
load("test_RAIN_df.RData",verbose=T)
head(in_df) # test OTU results. Data frame columns are equal to time points and rows equate to OTUs.
row.names(in_df)<-in_df$OTU.ID; in_df[1]<-NULL # Set OTU.IDs as row names
```

## Implementation
First step of implementation is data preparation. Because rain is a rank-based method, the results are pretty robust to a variety of input data distributions, but also we want to try and mitigate the effects of:
-linear trends in the timeseries, which mess up the mann-whitney tests
-the constrained negative covariance due to compositionality in the data, although this is difficult to truly mitigate without using ratios, which then lose interpretability for the rain analysis. We'll do CLR here and demonstrate what it does to the distribution of the data.

```{r Procedure}
# Centered log-ratio transformation. This is necessary due to the compositional nature of OTU results (see Gloor et al. 2017).
# More specifically, we want to do this because OTU/ASV tables are count data, meaning they can only be values between 0,inf. This is problematic for any statistical method that assumes equal variance, normally distributed data, or data with iid errors. The CLR moves the data from being between 0,inf to -inf,inf, similar to how log transforms do.
# ?clr()
df_clr<-as.data.frame(clr(in_df))
# See the difference between the raw counts
raw_counts<-hist(do.call('c',in_df),plot=F,breaks=500)
# Log-scaling the frequencies for easier graph viewing
raw_counts$counts<-log10(raw_counts$counts+1)
# Showing the frequency d'n
plot(raw_counts,
     xlab='Number of Counts',
     main='Count Data',
     ylab='Log10 Frequency')
# Doing the same for the transformed data
clr_hist_data<-hist(do.call('c',df_clr),plot=F,breaks=500)
clr_hist_data$counts<-log10(clr_hist_data$counts+1)
plot(clr_hist_data,
     xlab='CLR Transformed Value',
     main='CLR Transformation',
     ylab='Log10 Frequency')

# Detrend
# ?detrend()
# We detrend to remove possible linear dependencies in the time-series due to factors such as autocorrelation (example, a weekly trend). Detrending is just subtracting the linear regression of the time-series from the data. 
df_detr<-detrend(t(df_clr))
```
## Some heads up on the Method

```{r Rain on data}
# RAIN analysis
# ?rain()
# Set up RAIN parameters. Before implementing RAIN analysis, let's demonstrate the difference between the 'longitudinal' and 'independent' modes. The 'longitudinal' mode will not aggregate data taken at the same time of day and instead conduct a hypothesis test between each individual timepoint. The 'independent' mode will treat periods as independent, which results in conducting fewer tests with more replicates in each test group (for example, grouping the 4pm samples from days 1, 2, and 3 together and testing whether or not they are greater or less than the 6pm samples from days 1,2,3, instead of testing whether the 4pm sample from day 1 is greater than the 6 pm sample from day one, etc)
## Create some random time series data (SHOULD BE INSIGNIFICANT)
set.seed(398585)
random_time_series<-matrix(rnorm(1200),ncol=12)
## Now let's create some harmonic data with ~10% noise (SHOULD BE SIGNIFICANT)
set.seed(88973159)
timepoints<-1:12
harmonic_matrix<-t(matrix(rep(sin(timepoints*(pi/2)),100),nrow=12))
harmonic_matrix<-harmonic_matrix+rnorm(1200,sd=0.1)
## Now let's create some linearly increasing data with comparable noise (SHOULD BE INSIGNIFICANT)
set.seed(7341)
linear_time_series<-(1:12)/6 #making slope approx ~10-20% of magnitude
linear_matrix<-t(matrix(rep(linear_time_series,100),nrow=12))+rnorm(1200,sd=0.1)
## Finally, let's create some periodic data with a linear trend over the time series (SHOULD BE SIGNIFICANT)
set.seed(143164)
trend_and_noise_periodic<-t(matrix(rep(sin(timepoints*(pi/2))+linear_time_series/2,100),
                                   nrow=12))+rnorm(1200,sd=0.1)
## Demonstrating what detrending the trendy data makes it look like (SHOULD BE SIGNIFICANT)
dt_periodic<-t(detrend(t(trend_and_noise_periodic)))
## Now we say we expect a 24 hour period and we took measurements every 6 hrs for 3 days
all_synthetic_data<-rbind(random_time_series,
                          harmonic_matrix,
                          linear_matrix,
                          trend_and_noise_periodic,
                          dt_periodic)
rain_longitudinal<-rain(t(all_synthetic_data),period=24,deltat=6,method='longitudinal')
rain_indpendent<-rain(t(all_synthetic_data),period=24,deltat=6,method='independent')
## For visualization of differences make some labels
sample_type<-rep(c('random','harmonic','linear','trend_harmonic','detrended'),
                 each=100)
true_answer<-rep(c('no','periodic','no','periodic','periodic'),each=100)
rain_theory_frame<-data.frame(cbind(sample_type,true_answer),
                              long_mode=rain_longitudinal$pVal,
                              ind_mode=rain_indpendent$pVal)
ggplot(rain_theory_frame,aes(x=log10(ind_mode),y=log10(long_mode)))+
  geom_point(aes(col=sample_type,shape=true_answer))+
  ylab('Log10 p-value Longitudinal')+
  xlab('Log10 p-value Independent')+
  geom_abline(slope=1,intercept=0)
## Here we see the difference between the two modes. The solid line is the 1:1 line, which represents both methods having the same p-value result. Anything below this line was deemed lower p-value by longitudinal mode, anything above this line was deemed lower p-value by independent mode.
```
## Moving on to Real OTU data

```{r Rain On Real Data}
t<-(1:19)
ft <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
# Run RAIN
output_RAIN<-rain(as.matrix(df_detr), period=24, measure.sequence=ft, deltat=4, method="independent", na.rm = TRUE)

# Report results
head(output_RAIN[1:3,]) # Output data frame
hist(output_RAIN$pVal) # distribution of p-value results
sig<-subset(output_RAIN, pVal < 0.05); dim(sig)[1] # Total number of OTUs with significant rhymicity (p<0.05)
sig$OTU.ID<-row.names(sig); list_of_sig_OTUs<-unique(sig$OTU.ID) # list of OTUs with significant rhythmicity
```
## So how do I decide if a result is significant?
Important note: DECIDE THIS BEFORE YOU DO YOUR ANALYSIS, DON'T TRY ALL OF THESE CORRECTIONS AND THEN PICK THE ONE YOU LIKE. We demonstrate them all here for the purpose of introduction, but in the analysis performed in [cite original paper], significance criteria were decided before implementing analysis. 

```{r Significance Determination}
# Assessing significance
# Bonferroni correction
# This is the LEAST forgiving form of multiple testing correction (least power and least type I error)
bonf_sig<-subset(output_RAIN, pVal <= (0.05/nrow(output_RAIN)))

# Benjamini-Hochberg Correction
#?p.adjust()
fdr_ps<-p.adjust(output_RAIN$pVal,method='BH')
bh_sig<-output_RAIN[which(fdr_ps<=0.05),]

# Adaptive Benjamini-Hochberg Correction (see Benjamini and Hochberg 2001)
# Step 1: Sort p-values in ascending order
abh_ps<-output_RAIN$pVal[order(output_RAIN$pVal,decreasing=FALSE)]

# Step 2: Find p-values smaller than those expected at 5% FDR.
# Rejecting all null hypotheses which have p-values less than their corresponding abh_metric will
# yield the same results as p.adjust(method='BH') at the 0.05 significance level.
q<-0.05 #Significance level
m<-length(abh_ps) #Number of hypotheses tested
abh_metric<-q*(1:m)/m

#For a visualization of what this procedure does -- we consider all red points that fall below the black
# line significant
plot(1:m,abh_metric,type='l',
  xlab='p-value rank',
  ylab='p-value',lwd=2,
  main='Benjamini-Hochberg FDR Control')
points(1:m,abh_ps,col='red')
legend(650,0.05,fill=c('red','black'),legend=c('p-value','FDR controlled p threshold'))

# Step 3: Estimate the number of false null hypotheses according to the procedure outlined
# in Benjamini and Hochberg 2001
s=0
s_i=0
i=1
while(s>=s_i){
  s_i=s
  s=(1-abh_ps[i])/(1+m-i)
  i=i+1
}
m_0<-ceiling(1/s)+1

# Step 4: Find p-values smaller than those expected at 5% FDR for adjusted number of false hypotheses
abh_updated<-(1:length(abh_ps))*0.05/m_0
# For a similar visualization
plot(1:m,abh_updated,type='l',
     xlab='p-value rank',
     ylab='p-value',lwd=2,
     main='Adaptive Benjamini-Hochberg FDR Control')
points(1:m,abh_ps,col='red')
legend(650,0.05,fill=c('red','black'),legend=c('p-value','FDR controlled p threshold'))

# Step 5: Determine which tests resulted in a p-value significant at the 5% level considering FDR
abh_sigs<-output_RAIN[which(output_RAIN$pVal %in% abh_ps[which(abh_ps<=abh_updated)]),]

```


